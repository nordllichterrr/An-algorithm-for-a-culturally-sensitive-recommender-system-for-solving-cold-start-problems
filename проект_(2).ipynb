{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nordllichterrr/User-Cold-Start-Recommendation-System-Based-on-Hofstede-Cultural-Theory/blob/main/%D0%BF%D1%80%D0%BE%D0%B5%D0%BA%D1%82_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3oIWKUAyiFV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from charset_normalizer import from_path\n",
        "\n",
        "input_folder1 = r\"/content/dataset1\"\n",
        "input_folder2 = r\"/content/dataset2\"\n",
        "output_csv1_path = \"output_dataset1.csv\"\n",
        "output_csv2_path = \"output_dataset2.csv\"\n",
        "\n",
        "def detect_encoding(file_path):\n",
        "    result = from_path(file_path).best()\n",
        "    return result.encoding\n",
        "\n",
        "def process_folder(folder_path):\n",
        "    all_data = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            encoding = detect_encoding(file_path)\n",
        "            print(f\"Кодировка файла {filename}: {encoding}\")\n",
        "            with open(file_path, 'r', encoding=encoding) as file:\n",
        "                lines = file.readlines()\n",
        "                for line in lines:\n",
        "                    row = line.strip().split('\\t')\n",
        "                    all_data.append(row)\n",
        "    return all_data\n",
        "\n",
        "if os.path.exists(input_folder1):\n",
        "    print(f\"Обработка датасета 1: {input_folder1}\")\n",
        "    data1 = process_folder(input_folder1)\n",
        "    df1 = pd.DataFrame(data1)\n",
        "    df1.to_csv(output_csv1_path, index=False, header=False)\n",
        "    print(f\"Данные датасета 1 сохранены в {output_csv1_path}\")\n",
        "else:\n",
        "    print(f\"Папка {input_folder1} не найдена. Проверьте путь.\")\n",
        "\n",
        "if os.path.exists(input_folder2):\n",
        "    print(f\"Обработка датасета 2: {input_folder2}\")\n",
        "    data2 = process_folder(input_folder2)\n",
        "    df2 = pd.DataFrame(data2)\n",
        "    df2.to_csv(output_csv2_path, index=False, header=False)\n",
        "    print(f\"Данные датасета 2 сохранены в {output_csv2_path}\")\n",
        "else:\n",
        "    print(f\"Папка {input_folder2} не найдена. Проверьте путь.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCdNAhNf1KDR",
        "outputId": "135e5c3b-b9a0-433e-a190-4a445e5f1cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обработка датасета 1: /content/dataset1\n",
            "Кодировка файла wslist.txt: cp1250\n",
            "Кодировка файла rtMatrix.txt: ascii\n",
            "Кодировка файла tpMatrix.txt: ascii\n",
            "Кодировка файла userlist.txt: ascii\n",
            "Кодировка файла readme.txt: utf_8\n",
            "Данные датасета 1 сохранены в output_dataset1.csv\n",
            "Обработка датасета 2: /content/dataset2\n",
            "Кодировка файла rtdata.txt: ascii\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv('/content/output_dataset1.csv', header=None)\n",
        "df2 = pd.read_csv('/content/output_dataset2.csv', header=None)\n",
        "\n",
        "df1 = df1.drop_duplicates()\n",
        "df2 = df2.drop_duplicates()\n",
        "\n",
        "df1 = df1.dropna()\n",
        "df2 = df2.dropna()\n",
        "\n",
        "def remove_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "\n",
        "for col in df1.select_dtypes(include=[np.number]).columns:\n",
        "    df1 = remove_outliers(df1, col)\n",
        "\n",
        "for col in df2.select_dtypes(include=[np.number]).columns:\n",
        "    df2 = remove_outliers(df2, col)\n",
        "\n",
        "df1.to_csv('cleaned_dataset1.csv', index=False, header=False)\n",
        "df2.to_csv('cleaned_dataset2.csv', index=False, header=False)\n",
        "\n",
        "print(\"Данные очищены и сохранены в cleaned_dataset1.csv и cleaned_dataset2.csv\")"
      ],
      "metadata": {
        "id": "7rxniaBnyXM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "df1 = pd.read_csv('/content/cleaned_dataset1.csv', header=None)\n",
        "df2 = pd.read_csv('/content/cleaned_dataset2.csv', header=None)\n",
        "\n",
        "response_time_col = 0\n",
        "throughput_col = 1\n",
        "\n",
        "if response_time_col in df1.columns and throughput_col in df1.columns:\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    df1[[response_time_col, throughput_col]] = min_max_scaler.fit_transform(df1[[response_time_col, throughput_col]])\n",
        "\n",
        "if response_time_col in df2.columns and throughput_col in df2.columns:\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    df2[[response_time_col, throughput_col]] = min_max_scaler.fit_transform(df2[[response_time_col, throughput_col]])\n",
        "\n",
        "df1.to_csv('normalized_dataset1.csv', index=False, header=False)\n",
        "df2.to_csv('normalized_dataset2.csv', index=False, header=False)\n",
        "\n",
        "print(\"Данные нормализованы и сохранены в normalized_dataset1.csv и normalized_dataset2.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4I10Qi_0PCX",
        "outputId": "6c13a2d6-f637-43cd-e9d0-df02ac343315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Данные нормализованы и сохранены в normalized_dataset1.csv и normalized_dataset2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df1 = pd.read_csv('/content/normalized_dataset1.csv', header=None)\n",
        "df2 = pd.read_csv('/content/normalized_dataset2.csv', header=None)\n",
        "\n",
        "X1 = df1.iloc[:, :-1]  # Все столбцы, кроме последнего (признаки)\n",
        "y1 = df1.iloc[:, -1]    # Последний столбец (целевая переменная)\n",
        "X2 = df2.iloc[:, :-1]   # Все столбцы, кроме последнего (признаки)\n",
        "y2 = df2.iloc[:, -1]    # Последний столбец (целевая переменная)\n",
        "\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
        "\n",
        "X1_train['split'] = 'train'\n",
        "X1_test['split'] = 'test'\n",
        "X2_train['split'] = 'train'\n",
        "X2_test['split'] = 'test'\n",
        "\n",
        "train_test_data1 = pd.concat([X1_train, y1_train], axis=1)\n",
        "train_test_data1 = pd.concat([train_test_data1, pd.concat([X1_test, y1_test], axis=1)])\n",
        "\n",
        "train_test_data2 = pd.concat([X2_train, y2_train], axis=1)\n",
        "train_test_data2 = pd.concat([train_test_data2, pd.concat([X2_test, y2_test], axis=1)])\n",
        "\n",
        "train_test_data1.to_csv('train_test_data1.csv', index=False)\n",
        "train_test_data2.to_csv('train_test_data2.csv', index=False)\n",
        "\n",
        "print(\"Данные объединены и сохранены в train_test_data1.csv и train_test_data2.csv.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5awlHgM5IYR",
        "outputId": "d2c8c5e1-6609-410b-ef9e-b8d968317b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Данные объединены и сохранены в train_test_data1.csv и train_test_data2.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_test_data1 = pd.read_csv('/content/train_test_data1.csv')\n",
        "train_test_data2 = pd.read_csv('/content/train_test_data2.csv')\n",
        "\n",
        "train_test_data1['user_id'] = np.random.randint(1, 100, size=len(train_test_data1))\n",
        "train_test_data2['user_id'] = np.random.randint(1, 100, size=len(train_test_data2))\n",
        "\n",
        "train_test_data1['rating'] = np.random.randint(1, 6, size=len(train_test_data1))\n",
        "train_test_data2['rating'] = np.random.randint(1, 6, size=len(train_test_data2))\n",
        "\n",
        "train_test_data1['feature2'] = np.random.randint(1, 100, size=len(train_test_data1))\n",
        "train_test_data2['feature2'] = np.random.randint(1, 100, size=len(train_test_data2))\n",
        "\n",
        "print(\"train_test_data1:\")\n",
        "print(train_test_data1)\n",
        "\n",
        "print(\"\\ntrain_test_data2:\")\n",
        "print(train_test_data2)\n",
        "\n",
        "def save_updated_dataset(dataset, filename):\n",
        "    dataset.to_csv(filename, index=False)\n",
        "    print(f\"Датасет сохранён в файл: {filename}\")\n",
        "\n",
        "save_updated_dataset(train_test_data1, 'train_test_data1.csv')\n",
        "save_updated_dataset(train_test_data2, 'train_test_data2.csv')"
      ],
      "metadata": {
        "id": "PGM3Q_2j9Hvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "052f0a42-944f-4a73-e0b3-7b243b858317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_test_data1:\n",
            "          0         1      2      3      4      5      6      7      8      9  \\\n",
            "0  1.000000  0.679245  0.329  0.307  0.308  0.826  0.706  1.073  0.562  1.001   \n",
            "1  0.000000  0.000000  0.258  0.235  0.236  0.415  0.533  0.820  0.399  0.584   \n",
            "2  0.372197  1.000000  0.356  0.361  0.319  0.578  0.740  1.143  0.580  0.845   \n",
            "\n",
            "   ...   5819   5820   5821   5822   5823  split   5824  user_id  rating  \\\n",
            "0  ...  0.235  0.359  0.248  0.364  0.239  train  0.242       85       2   \n",
            "1  ...  0.188  0.141  0.144  0.142  0.174  train  0.141       36       2   \n",
            "2  ...  0.243  0.244  0.238  0.248  0.244   test  0.241       83       1   \n",
            "\n",
            "   feature2  \n",
            "0        37  \n",
            "1        79  \n",
            "2        64  \n",
            "\n",
            "[3 rows x 5829 columns]\n",
            "\n",
            "train_test_data2:\n",
            "         split                   0  user_id  rating  feature2\n",
            "0        train  6 787 23 1.4524715        6       4        51\n",
            "1        train    19 3107 17 0.112       54       4        38\n",
            "2        train      3 4273 8 0.055       30       3         5\n",
            "3        train    19 2615 36 3.364       37       3         6\n",
            "4        train      8 4068 7 0.444       59       3        15\n",
            "...        ...                 ...      ...     ...       ...\n",
            "8513456   test    13 4386 16 0.076       37       4        91\n",
            "8513457   test      0 1588 30 20.0       94       3        88\n",
            "8513458   test  6 3626 3 3.4785714       90       4        80\n",
            "8513459   test    11 3874 43 0.069       74       3        84\n",
            "8513460   test    16 3899 58 10.25       86       5        85\n",
            "\n",
            "[8513461 rows x 5 columns]\n",
            "Датасет сохранён в файл: train_test_data1.csv\n",
            "Датасет сохранён в файл: train_test_data2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Загружаем данные из файлов\n",
        "train_test_data1 = pd.read_csv('/content/train_test_data1.csv')\n",
        "train_test_data2 = pd.read_csv('/content/train_test_data2.csv')\n",
        "\n",
        "# Проверяем наличие столбцов 'feature1' и 'feature2'\n",
        "if 'feature1' not in train_test_data1.columns:\n",
        "    train_test_data1['feature1'] = np.random.rand(len(train_test_data1))\n",
        "if 'feature2' not in train_test_data1.columns:\n",
        "    train_test_data1['feature2'] = np.random.rand(len(train_test_data1))\n",
        "\n",
        "if 'feature1' not in train_test_data2.columns:\n",
        "    train_test_data2['feature1'] = np.random.rand(len(train_test_data2))\n",
        "if 'feature2' not in train_test_data2.columns:\n",
        "    train_test_data2['feature2'] = np.random.rand(len(train_test_data2))\n",
        "\n",
        "# Проверяем данные\n",
        "print(\"Данные train_test_data1:\")\n",
        "print(train_test_data1.head())\n",
        "\n",
        "print(\"\\nДанные train_test_data2:\")\n",
        "print(train_test_data2.head())\n",
        "\n",
        "def simulate_cold_start(data, test_size=0.2, keep_ratios=0.1):\n",
        "    user_ids = data['user_id'].unique()\n",
        "    new_users = np.random.choice(user_ids, size=int(test_size * len(user_ids)), replace=False)\n",
        "    new_users_data = data[data['user_id'].isin(new_users)].copy()\n",
        "    mask = np.random.rand(len(new_users_data)) > keep_ratios\n",
        "    new_users_data.loc[mask, 'rating'] = np.nan\n",
        "    existing_users_data = data[~data['user_id'].isin(new_users)]\n",
        "    return new_users_data, existing_users_data\n",
        "\n",
        "new_users_data1, existing_users_data1 = simulate_cold_start(train_test_data1)\n",
        "new_users_data2, existing_users_data2 = simulate_cold_start(train_test_data2)\n",
        "\n",
        "print(\"\\nРаспределение для train_test_data1:\")\n",
        "print(f\"Новые пользователи: {len(new_users_data1)} записей\")\n",
        "print(f\"Существующие пользователи: {len(existing_users_data1)} записей\")\n",
        "\n",
        "print(\"\\nРаспределение для train_test_data2:\")\n",
        "print(f\"Новые пользователи: {len(new_users_data2)} записей\")\n",
        "print(f\"Существующие пользователи: {len(existing_users_data2)} записей\")\n",
        "\n",
        "# Используем feature1 и feature2 как признаки, а rating как целевую переменную\n",
        "X_train = existing_users_data1[['feature1', 'feature2']]\n",
        "y_train = existing_users_data1['rating']\n",
        "\n",
        "X_test = new_users_data1[['feature1', 'feature2']]\n",
        "y_test = new_users_data1['rating']\n",
        "\n",
        "# Проверяем, что X_test не пустой\n",
        "if len(X_test) == 0:\n",
        "    print(\"\\nОшибка: Нет данных для тестирования (X_test пустой).\")\n",
        "    print(\"Возможные причины:\")\n",
        "    print(\"- В данных недостаточно новых пользователей.\")\n",
        "    print(\"- Все рейтинги новых пользователей были удалены.\")\n",
        "    print(\"Добавляем случайные данные для тестирования...\")\n",
        "\n",
        "    # Добавляем случайные данные для новых пользователей\n",
        "    X_test = np.random.rand(10, 2)  # 10 строк, 2 признака (feature1 и feature2)\n",
        "    y_test = np.random.randint(1, 6, size=10)  # Случайные рейтинги от 1 до 5\n",
        "    print(\"\\nСгенерировано 10 случайных строк для тестирования.\")\n",
        "else:\n",
        "    print(\"\\nДанные для тестирования найдены.\")\n",
        "\n",
        "# Обучаем модель (например, RandomForestClassifier)\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Предсказания на данных новых пользователей\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Оценка модели (если есть часть рейтингов для проверки)\n",
        "if len(y_test) > 0:\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\nТочность модели на новых пользователях: {accuracy:.2f}\")\n",
        "else:\n",
        "    print(\"\\nНет данных для оценки точности (все рейтинги новых пользователей отсутствуют).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2meVauFxS1e",
        "outputId": "559d6460-336f-40c9-aba1-64d11a08aa15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Данные train_test_data1:\n",
            "          0         1      2      3      4      5      6      7      8      9  \\\n",
            "0  1.000000  0.679245  0.329  0.307  0.308  0.826  0.706  1.073  0.562  1.001   \n",
            "1  0.000000  0.000000  0.258  0.235  0.236  0.415  0.533  0.820  0.399  0.584   \n",
            "2  0.372197  1.000000  0.356  0.361  0.319  0.578  0.740  1.143  0.580  0.845   \n",
            "\n",
            "   ...   5820   5821   5822   5823  split   5824  user_id  rating  feature2  \\\n",
            "0  ...  0.359  0.248  0.364  0.239  train  0.242       85       2        37   \n",
            "1  ...  0.141  0.144  0.142  0.174  train  0.141       36       2        79   \n",
            "2  ...  0.244  0.238  0.248  0.244   test  0.241       83       1        64   \n",
            "\n",
            "   feature1  \n",
            "0  0.512311  \n",
            "1  0.505269  \n",
            "2  0.984507  \n",
            "\n",
            "[3 rows x 5830 columns]\n",
            "\n",
            "Данные train_test_data2:\n",
            "   split                   0  user_id  rating  feature2  feature1\n",
            "0  train  6 787 23 1.4524715        6       4        51  0.567113\n",
            "1  train    19 3107 17 0.112       54       4        38  0.351439\n",
            "2  train      3 4273 8 0.055       30       3         5  0.405677\n",
            "3  train    19 2615 36 3.364       37       3         6  0.764155\n",
            "4  train      8 4068 7 0.444       59       3        15  0.698521\n",
            "\n",
            "Распределение для train_test_data1:\n",
            "Новые пользователи: 0 записей\n",
            "Существующие пользователи: 3 записей\n",
            "\n",
            "Распределение для train_test_data2:\n",
            "Новые пользователи: 1634128 записей\n",
            "Существующие пользователи: 6879333 записей\n",
            "\n",
            "Ошибка: Нет данных для тестирования (X_test пустой).\n",
            "Возможные причины:\n",
            "- В данных недостаточно новых пользователей.\n",
            "- Все рейтинги новых пользователей были удалены.\n",
            "Добавляем случайные данные для тестирования...\n",
            "\n",
            "Сгенерировано 10 случайных строк для тестирования.\n",
            "\n",
            "Точность модели на новых пользователях: 0.10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from tensorflow.keras.layers import Input, Embedding, Dot, Flatten, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Загрузка данных\n",
        "hofstede_file_path = ('/content/Hofstedes Dimensions Data.xlsx')\n",
        "hofstede_df = pd.read_excel(hofstede_file_path, sheet_name='6 dimensions for website')\n",
        "\n",
        "# Вычисление дисперсии для культурных измерений\n",
        "variance_data = {\n",
        "    'pdi': hofstede_df['pdi'].var(),\n",
        "    'idv': hofstede_df['idv'].var(),\n",
        "    'mas': hofstede_df['mas'].var(),\n",
        "    'uai': hofstede_df['uai'].var(),\n",
        "    'ltowvs': hofstede_df['ltowvs'].var(),\n",
        "    'ivr': hofstede_df['ivr'].var()\n",
        "}\n",
        "variance_df = pd.Series(variance_data)\n",
        "\n",
        "# Загрузка данных пользователей\n",
        "users_file_path = '/content/data.csv'\n",
        "qos_data = pd.read_csv(users_file_path)\n",
        "\n",
        "print(\"Исходные данные:\")\n",
        "print(qos_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tKNQtJ4sBfH",
        "outputId": "569f4d4c-069f-4e6f-c6e7-6d6055846ec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходные данные:\n",
            "   [User ID]    [IP Address]      [Country]    [IP No.]  \\\n",
            "0          0  12.108.127.138  United States   208437130   \n",
            "1          1    12.46.129.15  United States   204374287   \n",
            "2          2    122.1.115.91          Japan  2046915419   \n",
            "3          3    128.10.19.52  United States  2148143924   \n",
            "4          4    128.10.19.53  United States  2148143925   \n",
            "\n",
            "                                    [AS]  [Latitude]  [Longitude]  \n",
            "0             AS7018 AT&T Services, Inc.     38.0000     -97.0000  \n",
            "1             AS7018 AT&T Services, Inc.     38.0464    -122.2300  \n",
            "2  AS4713 NTT Communications Corporation     35.6850     139.7514  \n",
            "3                 AS17 Purdue University     40.4249     -86.9162  \n",
            "4                 AS17 Purdue University     40.4249     -86.9162  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразование данных в числовой формат\n",
        "qos_data['[User ID]'] = pd.to_numeric(qos_data['[User ID]'], errors='coerce')\n",
        "qos_data['[IP Address]'] = pd.to_numeric(qos_data['[IP Address]'], errors='coerce')\n",
        "qos_data['[Country]'] = pd.to_numeric(qos_data['[Country]'], errors='coerce')\n",
        "qos_data['[IP No.]'] = pd.to_numeric(qos_data['[IP No.]'], errors='coerce')\n",
        "qos_data['[AS]'] = pd.to_numeric(qos_data['[AS]'], errors='coerce')\n",
        "qos_data['[Latitude]'] = pd.to_numeric(qos_data['[Latitude]'], errors='coerce')\n",
        "qos_data['[Longitude]'] = pd.to_numeric(qos_data['[Longitude]'], errors='coerce')\n",
        "\n",
        "print(\"\\nДанные после преобразования в числовой формат:\")\n",
        "print(qos_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILoB8Sks7TP6",
        "outputId": "986f1402-51c8-40b5-8cae-8d2afa00b282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Данные после преобразования в числовой формат:\n",
            "   [User ID]  [IP Address]  [Country]    [IP No.]  [AS]  [Latitude]  \\\n",
            "0          0           NaN        NaN   208437130   NaN     38.0000   \n",
            "1          1           NaN        NaN   204374287   NaN     38.0464   \n",
            "2          2           NaN        NaN  2046915419   NaN     35.6850   \n",
            "3          3           NaN        NaN  2148143924   NaN     40.4249   \n",
            "4          4           NaN        NaN  2148143925   NaN     40.4249   \n",
            "\n",
            "   [Longitude]  \n",
            "0     -97.0000  \n",
            "1    -122.2300  \n",
            "2     139.7514  \n",
            "3     -86.9162  \n",
            "4     -86.9162  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Удаление столбцов, полностью состоящих из NaN\n",
        "columns_with_all_nan = [col for col in qos_data.columns if qos_data[col].isna().all()]\n",
        "qos_data = qos_data.drop(columns=columns_with_all_nan)\n",
        "\n",
        "if columns_with_all_nan:\n",
        "    print(f\"\\nУдалены столбцы, полностью состоящие из NaN: {columns_with_all_nan}\")\n",
        "else:\n",
        "    print(\"\\nСтолбцы, полностью состоящие из NaN, отсутствуют.\")\n",
        "\n",
        "print(\"\\nДанные после удаления столбцов с NaN:\")\n",
        "print(qos_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucfUpH-_B_Od",
        "outputId": "41d63c9b-b386-4d4e-c2b9-d4f14e4e87b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Удалены столбцы, полностью состоящие из NaN: ['[IP Address]', '[Country]', '[AS]']\n",
            "\n",
            "Данные после удаления столбцов с NaN:\n",
            "   [User ID]    [IP No.]  [Latitude]  [Longitude]\n",
            "0          0   208437130     38.0000     -97.0000\n",
            "1          1   204374287     38.0464    -122.2300\n",
            "2          2  2046915419     35.6850     139.7514\n",
            "3          3  2148143924     40.4249     -86.9162\n",
            "4          4  2148143925     40.4249     -86.9162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделение данных на числовые и нечисловые столбцы\n",
        "qos_data_numeric = qos_data.select_dtypes(include=[np.number])\n",
        "qos_data_non_numeric = qos_data.select_dtypes(exclude=[np.number])\n",
        "\n",
        "print(\"\\nЧисловые столбцы:\")\n",
        "print(qos_data_numeric.head())\n",
        "\n",
        "print(\"\\nНечисловые столбцы:\")\n",
        "print(qos_data_non_numeric.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXGRJViTCDIe",
        "outputId": "b7156eca-0862-4c87-cd69-523e00485caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Числовые столбцы:\n",
            "   [User ID]    [IP No.]  [Latitude]  [Longitude]\n",
            "0          0   208437130     38.0000     -97.0000\n",
            "1          1   204374287     38.0464    -122.2300\n",
            "2          2  2046915419     35.6850     139.7514\n",
            "3          3  2148143924     40.4249     -86.9162\n",
            "4          4  2148143925     40.4249     -86.9162\n",
            "\n",
            "Нечисловые столбцы:\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: [0, 1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка данных на пропущенные значения\n",
        "if qos_data_numeric.isnull().any().any():\n",
        "    print(\"\\nОбнаружены пропущенные значения. Заполняем с использованием KNNImputer...\")\n",
        "\n",
        "    # Использование KNNImputer для заполнения пропущенных значений\n",
        "    imputer = KNNImputer(n_neighbors=5)  # Количество соседей можно настроить\n",
        "    qos_data_imputed = imputer.fit_transform(qos_data_numeric)\n",
        "\n",
        "    # Преобразование обратно в DataFrame\n",
        "    qos_data_imputed_df = pd.DataFrame(qos_data_imputed, columns=qos_data_numeric.columns)\n",
        "\n",
        "    print(\"\\nДанные после заполнения пропусков:\")\n",
        "    print(qos_data_imputed_df.head())\n",
        "\n",
        "    # Объединение с нечисловыми столбцами\n",
        "    if not qos_data_non_numeric.empty:\n",
        "        qos_data = pd.concat([qos_data_imputed_df, qos_data_non_numeric.reset_index(drop=True)], axis=1)\n",
        "    else:\n",
        "        qos_data = qos_data_imputed_df\n",
        "\n",
        "    print(\"\\nДанные после объединения:\")\n",
        "    print(qos_data.head())\n",
        "else:\n",
        "    print(\"\\nПропущенные значения отсутствуют.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ok2dJcuCHQ_",
        "outputId": "624199a3-c448-42b9-a48a-8236fd919a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Пропущенные значения отсутствуют.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка данных после очистки\n",
        "if qos_data.empty:\n",
        "    raise ValueError(\"Данные пусты после очистки.\")\n",
        "\n",
        "if qos_data.isnull().any().any():\n",
        "    raise ValueError(\"Данные содержат пропущенные значения.\")\n",
        "\n",
        "# Сохранение данных в файл (опционально)\n",
        "qos_data.to_csv('processed_qos_data.csv', index=False)"
      ],
      "metadata": {
        "id": "IOu8PoAECKqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "qos_data = pd.read_csv('processed_qos_data.csv')\n",
        "\n",
        "if qos_data.empty:\n",
        "    raise ValueError(\"Данные пусты после очистки.\")\n",
        "\n",
        "if qos_data.isnull().any().any():\n",
        "    print(\"Внимание: Данные содержат пропущенные значения. Строки с пропусками:\")\n",
        "    print(qos_data[qos_data.isnull().any(axis=1)])\n",
        "    qos_data = qos_data.dropna()\n",
        "\n",
        "print(\"\\nДанные после обработки:\")\n",
        "print(qos_data.head())\n",
        "\n",
        "qos_data.to_csv('cleaned_qos_data.csv', index=False)\n",
        "print(\"\\nОбработанные данные сохранены в файл 'cleaned_qos_data.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fot2BvmqcOUe",
        "outputId": "582235a9-0e41-467e-e315-f50d11de76b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Данные после обработки:\n",
            "   [User ID]    [IP No.]  [Latitude]  [Longitude]\n",
            "0          0   208437130     38.0000     -97.0000\n",
            "1          1   204374287     38.0464    -122.2300\n",
            "2          2  2046915419     35.6850     139.7514\n",
            "3          3  2148143924     40.4249     -86.9162\n",
            "4          4  2148143925     40.4249     -86.9162\n",
            "\n",
            "Обработанные данные сохранены в файл 'cleaned_qos_data.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Input, Embedding, Concatenate, Flatten, Dense, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Загрузка данных\n",
        "qos_data = pd.read_csv('/content/cleaned_qos_data.csv')\n",
        "\n",
        "# Проверка на пропущенные значения в датасете\n",
        "print(\"Пропущенные значения в датасете:\")\n",
        "print(qos_data.isnull().sum())\n",
        "\n",
        "# Заполнение пропущенных значений\n",
        "qos_data = qos_data.fillna(0)  # Замените на более подходящую стратегию, если необходимо\n",
        "\n",
        "# Преобразование [User ID] в int\n",
        "qos_data['[User ID]'] = qos_data['[User ID]'].astype(int)\n",
        "\n",
        "# Преобразование [Country], [IP Address] и [IP No.] в категориальные индексы\n",
        "label_encoder_country = LabelEncoder()\n",
        "label_encoder_ip = LabelEncoder()\n",
        "label_encoder_ipno = LabelEncoder()\n",
        "\n",
        "qos_data['[Country]'] = label_encoder_country.fit_transform(qos_data['[Country]'])\n",
        "qos_data['[IP Address]'] = label_encoder_ip.fit_transform(qos_data['[IP Address]'])\n",
        "qos_data['[IP No.]'] = label_encoder_ipno.fit_transform(qos_data['[IP No.]'])\n",
        "\n",
        "# Убедимся, что индексация начинается с 0\n",
        "qos_data['[User ID]'] = qos_data['[User ID]'] - qos_data['[User ID]'].min()\n",
        "qos_data['[Country]'] = qos_data['[Country]'] - qos_data['[Country]'].min()\n",
        "qos_data['[IP Address]'] = qos_data['[IP Address]'] - qos_data['[IP Address]'].min()\n",
        "qos_data['[IP No.]'] = qos_data['[IP No.]'] - qos_data['[IP No.]'].min()\n",
        "\n",
        "# Уникальные значения для Embedding слоев\n",
        "num_users = qos_data['[User ID]'].max() + 1  # Максимальный индекс + 1\n",
        "num_ipno = qos_data['[IP No.]'].max() + 1\n",
        "num_lat = qos_data['[Latitude]'].max() + 1\n",
        "num_longi = qos_data['[Longitude]'].max() + 1\n",
        "num_countries = qos_data['[Country]'].max() + 1\n",
        "num_ip_addresses = qos_data['[IP Address]'].max() + 1\n",
        "embedding_size = 10\n",
        "\n",
        "# Проверка уникальных значений\n",
        "print(\"Максимальный индекс User ID:\", qos_data['[User ID]'].max())\n",
        "print(\"Максимальный индекс IP No.:\", qos_data['[IP No.]'].max())\n",
        "print(\"Максимальный индекс Country:\", qos_data['[Country]'].max())\n",
        "print(\"Максимальный индекс IP Address:\", qos_data['[IP Address]'].max())\n",
        "\n",
        "# Создание модели\n",
        "def create_model(num_users, num_ipno, num_lat, num_longi, num_countries, num_ip_addresses, embedding_size):\n",
        "    # Входные слои\n",
        "    user_input = Input(shape=(1,), name='user_input')\n",
        "    ipno_input = Input(shape=(1,), name='ipno_input')\n",
        "    lat_input = Input(shape=(1,), name='lat_input')\n",
        "    longi_input = Input(shape=(1,), name='longi_input')\n",
        "    country_input = Input(shape=(1,), name='country_input')\n",
        "    ip_address_input = Input(shape=(1,), name='ip_address_input')\n",
        "\n",
        "    # Embedding слои для категориальных данных\n",
        "    user_embedding = Embedding(input_dim=num_users, output_dim=embedding_size, name='user_embedding')(user_input)\n",
        "    ipno_embedding = Embedding(input_dim=num_ipno, output_dim=embedding_size, name='ipno_embedding')(ipno_input)\n",
        "    country_embedding = Embedding(input_dim=num_countries, output_dim=embedding_size, name='country_embedding')(country_input)\n",
        "    ip_address_embedding = Embedding(input_dim=num_ip_addresses, output_dim=embedding_size, name='ip_address_embedding')(ip_address_input)\n",
        "\n",
        "    # Приведение Embedding слоев к размерности (batch_size, embedding_size)\n",
        "    user_embedding = Flatten()(user_embedding)\n",
        "    ipno_embedding = Flatten()(ipno_embedding)\n",
        "    country_embedding = Flatten()(country_embedding)\n",
        "    ip_address_embedding = Flatten()(ip_address_embedding)\n",
        "\n",
        "    # Приведение числовых данных к размерности (batch_size, embedding_size)\n",
        "    lat_reshaped = Reshape((1,))(lat_input)\n",
        "    longi_reshaped = Reshape((1,))(longi_input)\n",
        "\n",
        "    lat_dense = Dense(embedding_size, activation='relu', name='lat_dense')(lat_reshaped)\n",
        "    longi_dense = Dense(embedding_size, activation='relu', name='longi_dense')(longi_reshaped)\n",
        "\n",
        "    # Конкатенация всех слоев\n",
        "    concatenated = Concatenate()([user_embedding, ipno_embedding, lat_dense, longi_dense, country_embedding, ip_address_embedding])\n",
        "\n",
        "    # Добавляем несколько скрытых слоев для лучшего обучения\n",
        "    hidden1 = Dense(64, activation='relu')(concatenated)\n",
        "    hidden2 = Dense(32, activation='relu')(hidden1)\n",
        "\n",
        "    # Выходной слой (предполагаем, что предсказываем некоторую метрику QoS)\n",
        "    output = Dense(1, activation='linear', name='output')(hidden2)\n",
        "\n",
        "    # Создание модели\n",
        "    model = Model(inputs=[user_input, ipno_input, lat_input, longi_input, country_input, ip_address_input], outputs=output)\n",
        "    return model\n",
        "\n",
        "# Определение целевой переменной (необходимо указать столбец для предсказания)\n",
        "# Предположим, что мы предсказываем качество обслуживания (QoS), как столбец '[QoS]'\n",
        "target_variable = 'QoS'  # Замените на реальное имя столбца\n",
        "if target_variable in qos_data.columns:\n",
        "    y = qos_data[target_variable].values\n",
        "else:\n",
        "    # Если в данных нет столбца с целевой переменной, можно создать синтетическую для демонстрации\n",
        "    print(f\"Предупреждение: столбец '{target_variable}' не найден. Создаем синтетическую целевую переменную.\")\n",
        "    y = np.random.rand(len(qos_data))\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "train_data, test_data, y_train, y_test = train_test_split(qos_data, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Подготовка входных данных для обучения\n",
        "user_ids_train = train_data['[User ID]'].values.reshape(-1, 1)\n",
        "ipno_values_train = train_data['[IP No.]'].values.reshape(-1, 1)\n",
        "lat_values_train = train_data['[Latitude]'].values.reshape(-1, 1)\n",
        "longi_values_train = train_data['[Longitude]'].values.reshape(-1, 1)\n",
        "country_values_train = train_data['[Country]'].values.reshape(-1, 1)\n",
        "ip_address_values_train = train_data['[IP Address]'].values.reshape(-1, 1)\n",
        "\n",
        "# Подготовка входных данных для тестирования\n",
        "user_ids_test = test_data['[User ID]'].values.reshape(-1, 1)\n",
        "ipno_values_test = test_data['[IP No.]'].values.reshape(-1, 1)\n",
        "lat_values_test = test_data['[Latitude]'].values.reshape(-1, 1)\n",
        "longi_values_test = test_data['[Longitude]'].values.reshape(-1, 1)\n",
        "country_values_test = test_data['[Country]'].values.reshape(-1, 1)\n",
        "ip_address_values_test = test_data['[IP Address]'].values.reshape(-1, 1)\n",
        "\n",
        "# Создание и компиляция модели\n",
        "model = create_model(num_users, num_ipno, num_lat, num_longi, num_countries, num_ip_addresses, embedding_size)\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Вывод сводки модели\n",
        "model.summary()\n",
        "\n",
        "# Обучение модели\n",
        "print(\"\\nОбучение модели...\")\n",
        "history = model.fit(\n",
        "    [user_ids_train, ipno_values_train, lat_values_train, longi_values_train, country_values_train, ip_address_values_train],\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Оценка модели на тестовых данных\n",
        "print(\"\\nОценка модели на тестовых данных:\")\n",
        "test_loss, test_mae = model.evaluate(\n",
        "    [user_ids_test, ipno_values_test, lat_values_test, longi_values_test, country_values_test, ip_address_values_test],\n",
        "    y_test,\n",
        "    verbose=1\n",
        ")\n",
        "print(f\"Тестовая потеря (MSE): {test_loss}\")\n",
        "print(f\"Тестовая средняя абсолютная ошибка (MAE): {test_mae}\")\n",
        "\n",
        "# Предсказание\n",
        "print(\"\\nПримеры предсказаний:\")\n",
        "predictions = model.predict(\n",
        "    [user_ids_test[:5], ipno_values_test[:5], lat_values_test[:5], longi_values_test[:5], country_values_test[:5], ip_address_values_test[:5]]\n",
        ")\n",
        "\n",
        "# Проверка формы predictions\n",
        "print(\"Форма predictions:\", predictions.shape)\n",
        "\n",
        "# Если predictions имеет форму (n_samples, 1), преобразуем её в (n_samples,)\n",
        "if predictions.shape[1] == 1:\n",
        "    predictions = predictions.flatten()\n",
        "\n",
        "# Проверка формы y_test\n",
        "print(\"Форма y_test:\", y_test.shape)\n",
        "\n",
        "# Если y_test имеет форму (n_samples, 1), преобразуем её в (n_samples,)\n",
        "if len(y_test.shape) > 1 and y_test.shape[1] == 1:\n",
        "    y_test = y_test.flatten()\n",
        "\n",
        "# Проверка размеров массивов\n",
        "print(f\"Размер predictions: {len(predictions)}, Размер y_test: {len(y_test)}\")\n",
        "\n",
        "# Корректный цикл с учетом длины массивов\n",
        "for i in range(min(5, len(predictions), len(y_test))):\n",
        "    print(f\"Пример {i}: Предсказанное значение: {predictions[i]}, Фактическое значение: {y_test[i]}\")\n",
        "\n",
        "# Визуализация результатов обучения\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Потери при обучении')\n",
        "plt.xlabel('Эпоха')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mae'], label='Train MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.title('Средняя абсолютная ошибка')\n",
        "plt.xlabel('Эпоха')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rTzOUBWociKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class CulturalSVDpp(object):\n",
        "    def __init__(self, alpha, reg_p, reg_q, reg_bu, reg_bi, reg_cultural, number_LatentFactors=10, number_epochs=10,\n",
        "                 columns=[\"userId\", \"movieId\", \"rating\", \"country\"], cultural_dim_weights=[1/6]*6):\n",
        "        self.alpha = alpha\n",
        "        self.reg_p = reg_p\n",
        "        self.reg_q = reg_q\n",
        "        self.reg_bu = reg_bu\n",
        "        self.reg_bi = reg_bi\n",
        "        self.reg_cultural = reg_cultural\n",
        "        self.number_LatentFactors = number_LatentFactors\n",
        "        self.number_epochs = number_epochs\n",
        "        self.columns = columns\n",
        "        self.cultural_dim_weights = cultural_dim_weights\n",
        "\n",
        "    def fit(self, dataset, valset, cultural_data):\n",
        "        self.dataset = pd.DataFrame(dataset)\n",
        "        self.valset = valset\n",
        "        self.cultural_data = cultural_data\n",
        "\n",
        "        self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]]\n",
        "        self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]]\n",
        "        self.globalMean = self.dataset[self.columns[2]].mean()\n",
        "\n",
        "        self.P, self.Q, self.bu, self.bi, self.Y, self.C = self.sgd()\n",
        "\n",
        "    def _init_matrix(self):\n",
        "        P = dict(zip(\n",
        "            self.users_ratings.index,\n",
        "            np.random.rand(len(self.users_ratings), self.number_LatentFactors).astype(np.float32)\n",
        "        ))\n",
        "        Q = dict(zip(\n",
        "            self.items_ratings.index,\n",
        "            np.random.rand(len(self.items_ratings), self.number_LatentFactors).astype(np.float32)\n",
        "        ))\n",
        "\n",
        "        countries = self.dataset[self.columns[3]].unique()\n",
        "        C = dict(zip(\n",
        "            countries,\n",
        "            np.random.rand(len(countries), self.number_LatentFactors).astype(np.float32)\n",
        "        ))\n",
        "        return P, Q, C\n",
        "\n",
        "    def cultural_distance(self, country1, country2):\n",
        "        if country1 not in self.cultural_data or country2 not in self.cultural_data:\n",
        "            return 0.0\n",
        "        country1_data = self.cultural_data[country1]\n",
        "        country2_data = self.cultural_data[country2]\n",
        "\n",
        "        distance = 0.0\n",
        "        m = len(country1_data)\n",
        "        if m != len(country2_data):\n",
        "            raise ValueError(\"Cultural dimension vectors must have the same length\")\n",
        "\n",
        "        for i in range(m):\n",
        "            # Учёт весов для каждого культурного измерения\n",
        "            distance += (self.cultural_dim_weights[i] * (country1_data[i] - country2_data[i])**2)\n",
        "\n",
        "        return np.sqrt(distance)\n",
        "\n",
        "    def predict(self, uid, iid):\n",
        "        if uid not in self.users_ratings.index or iid not in self.items_ratings.index:\n",
        "            return self.globalMean\n",
        "        p_u = self.P[uid]\n",
        "        q_i = self.Q[iid]\n",
        "        Y = self.Y\n",
        "        _sum_yj = np.zeros([1, self.number_LatentFactors])\n",
        "        jids = self.users_ratings.loc[uid]['movieId'][0]\n",
        "        Nu = len(jids)\n",
        "        for jid in jids:\n",
        "            _sum_yj += Y[jid]\n",
        "\n",
        "        # Культурный компонент:\n",
        "        user_country = self.dataset[self.dataset[self.columns[0]] == uid][self.columns[3]].iloc[0]\n",
        "        item_country = self.dataset[self.dataset[self.columns[1]] == iid][self.columns[3]].iloc[0]\n",
        "\n",
        "        c_u = self.C[user_country]\n",
        "        c_i = self.C[item_country]\n",
        "        cultural_component = np.dot(c_u, c_i)\n",
        "\n",
        "        return self.globalMean + self.bu[uid] + self.bi[iid] + np.dot(p_u + np.sqrt(1 / Nu) * _sum_yj, q_i) + cultural_component\n",
        "\n",
        "    def test(self, testset):\n",
        "        for uid, iid, real_rating, country in testset.itertuples(index=False):\n",
        "            try:\n",
        "                pred_rating = self.predict(uid, iid)\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "            else:\n",
        "                yield uid, iid, real_rating, pred_rating\n",
        "\n",
        "    def accuracy(self, predict_results):\n",
        "        def rmse_mae(predict_results):\n",
        "            length = 0\n",
        "            _rmse_sum = 0\n",
        "            _mae_sum = 0\n",
        "            for uid, iid, real_rating, pred_rating in predict_results:\n",
        "                length += 1\n",
        "                _rmse_sum += (pred_rating - real_rating) ** 2\n",
        "                _mae_sum += abs(pred_rating - real_rating)\n",
        "            return np.sqrt(_rmse_sum / length), _mae_sum / length\n",
        "\n",
        "        return rmse_mae(predict_results)\n",
        "\n",
        "    def sgd(self):\n",
        "        P, Q, C = self._init_matrix()\n",
        "\n",
        "        # Инициализация смещений пользователей и элементов\n",
        "        bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings))))\n",
        "        bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings))))\n",
        "        Y = dict(zip(\n",
        "            self.items_ratings.index,\n",
        "            np.random.rand(len(self.items_ratings), self.number_LatentFactors).astype(np.float32)\n",
        "        ))\n",
        "\n",
        "        rmse_list = []\n",
        "        mae_list = []\n",
        "\n",
        "        for i in range(self.number_epochs):\n",
        "            print(\"iter%d\" % i)\n",
        "            error_list = []\n",
        "            for uid, iid, r_ui, country in self.dataset.itertuples(index=False):\n",
        "\n",
        "                jids = self.users_ratings.loc[uid]['movieId'][0]\n",
        "                Nu = len(jids)\n",
        "                _sum_yj = np.zeros([self.number_LatentFactors])\n",
        "\n",
        "                for jid in jids:\n",
        "                    _sum_yj += Y[jid]\n",
        "\n",
        "                # Культурный компонент:\n",
        "                user_country = self.dataset[self.dataset[self.columns[0]] == uid][self.columns[3]].iloc[0]\n",
        "                item_country = self.dataset[self.dataset[self.columns[1]] == iid][self.columns[3]].iloc[0]\n",
        "                c_u = C[user_country]\n",
        "                c_i = C[item_country]\n",
        "\n",
        "                # Предсказание:\n",
        "                pred = self.globalMean + bu[uid] + bi[iid] + np.dot(P[uid] + np.sqrt(1 / Nu) * _sum_yj, Q[iid]) + np.dot(c_u, c_i)\n",
        "                err = np.float32(r_ui - pred)\n",
        "\n",
        "                # Обновление параметров:\n",
        "                for jid in jids:\n",
        "                    Y[jid] += self.alpha * (err * np.sqrt(1 / Nu) * Q[iid] - 0.01 * Y[jid])\n",
        "\n",
        "                P[uid] += self.alpha * (err * Q[iid] - self.reg_p * P[uid])\n",
        "                Q[iid] += self.alpha * (err * (P[uid] + np.sqrt(1 / Nu) * _sum_yj) - self.reg_q * Q[iid])\n",
        "\n",
        "                bu[uid] += self.alpha * (err - self.reg_bu * bu[uid])\n",
        "                bi[iid] += self.alpha * (err - self.reg_bi * bi[iid])\n",
        "\n",
        "                # Обновление культурных факторов:\n",
        "                C[user_country] += self.alpha * (err * c_i - self.reg_cultural * c_u)\n",
        "                C[item_country] += self.alpha * (err * c_u - self.reg_cultural * c_i)\n",
        "\n",
        "                error_list.append(err ** 2)\n",
        "            print(np.sqrt(np.mean(error_list)))\n",
        "            self.P = P\n",
        "            self.Q = Q\n",
        "            self.bu = bu\n",
        "            self.bi = bi\n",
        "            self.Y = Y\n",
        "            self.C = C\n",
        "\n",
        "            pred_results = self.test(self.valset)\n",
        "            rmse, mae = self.accuracy(pred_results)\n",
        "            rmse_list.append(rmse)\n",
        "            mae_list.append(mae)\n",
        "            print(\"rmse: \", rmse, \"mae: \", mae)\n",
        "\n",
        "        x = range(1, self.number_epochs + 1)\n",
        "        plt.plot(x, rmse_list)\n",
        "        plt.title('SVDpp_SGD with Cultural Component')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('RMSE')\n",
        "        plt.show()\n",
        "        return P, Q, bu, bi, Y, C\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    cultural_data = pd.read_excel('/content/Hofstedes Dimensions Data.xlsx')\n",
        "    trainset = pd.read_csv('/content/cleaned_qos_data.csv')\n",
        "    # Параметры\n",
        "    alpha = 0.01\n",
        "    reg_p = 0.01\n",
        "    reg_q = 0.01\n",
        "    reg_bu = 0.01\n",
        "    reg_bi = 0.01\n",
        "    reg_cultural = 0.01\n",
        "    number_LatentFactors = 10\n",
        "    number_epochs = 10\n",
        "    cultural_dim_weights = [1/6]*6\n",
        "    columns=[\"userId\", \"movieId\", \"rating\", \"country\"]\n",
        "\n",
        "    # Инициализация и обучение модели\n",
        "    algo = CulturalSVDpp(alpha, reg_p, reg_q, reg_bu, reg_bi, reg_cultural, number_LatentFactors, number_epochs, columns, cultural_dim_weights)\n",
        "    algo.fit(trainset, valset, cultural_data)\n",
        "\n",
        "    # Оценка модели\n",
        "    pred_results = algo.test(valset)\n",
        "    rmse, mae = algo.accuracy(pred_results)\n",
        "    print(\"rmse: \", rmse, \"mae: \", mae)\n"
      ],
      "metadata": {
        "id": "CCdCa8OzRUPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class CulturalSVDpp(object):\n",
        "    def __init__(self, alpha, reg_p, reg_q, reg_bu, reg_bi, reg_cultural, number_LatentFactors=10, number_epochs=10, K=10,\n",
        "                 columns=[\"userId\", \"movieId\", \"rating\", \"country\"], cultural_dim_weights=[1/6]*6, epsilon2=0.5, epsilon3=0.5, cultural_dimension_variances=[1]*6):\n",
        "        self.alpha = alpha\n",
        "        self.reg_p = reg_p\n",
        "        self.reg_q = reg_q\n",
        "        self.reg_bu = reg_bu\n",
        "        self.reg_bi = reg_bi\n",
        "        self.reg_cultural = reg_cultural\n",
        "        self.number_LatentFactors = number_LatentFactors\n",
        "        self.number_epochs = number_epochs\n",
        "        self.K = K\n",
        "        self.columns = columns\n",
        "        self.cultural_dim_weights = cultural_dim_weights\n",
        "        self.epsilon2 = epsilon2\n",
        "        self.epsilon3 = epsilon3\n",
        "        self.cultural_dimension_variances = cultural_dimension_variances\n",
        "\n",
        "    def fit(self, dataset, valset, cultural_data):\n",
        "        self.dataset = pd.DataFrame(dataset)\n",
        "        self.valset = valset\n",
        "        self.cultural_data = cultural_data\n",
        "\n",
        "        self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]]\n",
        "        self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]]\n",
        "        self.globalMean = self.dataset[self.columns[2]].mean()\n",
        "\n",
        "        self.P, self.Q, self.bu, self.bi, self.Y, self.C = self.sgd()\n",
        "\n",
        "    def _init_matrix(self):\n",
        "        P = dict(zip(\n",
        "            self.users_ratings.index,\n",
        "            np.random.rand(len(self.users_ratings), self.number_LatentFactors).astype(np.float32)\n",
        "        ))\n",
        "        Q = dict(zip(\n",
        "            self.items_ratings.index,\n",
        "            np.random.rand(len(self.items_ratings), self.number_LatentFactors).astype(np.float32)\n",
        "        ))\n",
        "\n",
        "        countries = self.dataset[self.columns[3]].unique()\n",
        "        C = dict(zip(\n",
        "            countries,\n",
        "            np.random.rand(len(countries), self.number_LatentFactors).astype(np.float32)\n",
        "        ))\n",
        "        return P, Q, C\n",
        "\n",
        "    def cultural_distance(self, country1, country2):\n",
        "        if country1 not in self.cultural_data or country2 not in self.cultural_data:\n",
        "            return 0.0\n",
        "\n",
        "        country1_data = self.cultural_data[country1]\n",
        "        country2_data = self.cultural_data[country2]\n",
        "\n",
        "        distance = 0.0\n",
        "        m = len(country1_data)\n",
        "        if m != len(country2_data):\n",
        "            raise ValueError(\"Cultural dimension vectors must have the same length\")\n",
        "\n",
        "        for i in range(m):\n",
        "            distance += (self.cultural_dim_weights[i] * self.cultural_dimension_variances[i] * (country1_data[i] - country2_data[i])**2)\n",
        "\n",
        "        return np.sqrt(distance)\n",
        "\n",
        "    def cultural_similarity(self, country1, country2):\n",
        "        cd1 = self.cultural_distance(country1, country1)\n",
        "        cd2 = self.cultural_distance(country2, country2)\n",
        "        return 1 - (cd1 + cd2)\n",
        "\n",
        "    def user_preference(self, user_country, service_country):\n",
        "        cdu = self.cultural_distance(user_country, user_country)\n",
        "        cds = self.cultural_distance(service_country, service_country)\n",
        "        return abs(cdu - cds)\n",
        "\n",
        "    def select_heuristic_services(self, new_user_country):\n",
        "        similar_users = []\n",
        "        for user in self.users_ratings.index:\n",
        "            user_country = self.dataset[self.dataset[self.columns[0]] == user][self.columns[3]].iloc[0]\n",
        "            similarity = self.cultural_similarity(new_user_country, user_country)\n",
        "            similar_users.append((user, similarity))\n",
        "\n",
        "        similar_users = sorted(similar_users, key=lambda x: x[1], reverse=True)\n",
        "        similar_users = [user for user, similarity in similar_users]\n",
        "\n",
        "        preferred_services = set()\n",
        "        for user in similar_users:\n",
        "            user_country = self.dataset[self.dataset[self.columns[0]] == user][self.columns[3]].iloc[0]\n",
        "            user_services = self.dataset[self.dataset[self.columns[0]] == user][self.columns[1]].tolist()\n",
        "            for service in user_services:\n",
        "                service_country = self.dataset[self.dataset[self.columns[1]] == service][self.columns[3]].iloc[0]\n",
        "                preference = self.user_preference(user_country, service_country)\n",
        "                if preference < self.epsilon2:\n",
        "                    preferred_services.add(service)\n",
        "\n",
        "        preferred_services = list(preferred_services)\n",
        "\n",
        "        if len(preferred_services) < self.K:\n",
        "            additional_services = set()\n",
        "            for service in preferred_services:\n",
        "                service_country = self.dataset[self.dataset[self.columns[1]] == service][self.columns[3]].iloc[0]\n",
        "                for other_service in self.items_ratings.index:\n",
        "                    other_service_country = self.dataset[self.dataset[self.columns[1]] == other_service][self.columns[3]].iloc[0]\n",
        "                    similarity = self.cultural_similarity(service_country, other_service_country)\n",
        "                    if similarity > self.epsilon3:\n",
        "                        additional_services.add(other_service)\n",
        "\n",
        "            preferred_services.extend(list(additional_services))\n",
        "\n",
        "        return preferred_services[:self.K]\n",
        "\n",
        "    def predict(self, uid, iid):\n",
        "        if uid not in self.users_ratings.index or iid not in self.items_ratings.index:\n",
        "            user_country = self.dataset[self.dataset[self.columns[0]] == uid][self.columns[3]].iloc[0]\n",
        "            heuristic_services = self.select_heuristic_services(user_country)\n",
        "            if not heuristic_services:\n",
        "                return self.globalMean\n",
        "\n",
        "            qos_predictions = []\n",
        "            for service in heuristic_services:\n",
        "                if service in self.items_ratings.index:\n",
        "                    users_of_service = self.dataset[self.dataset[self.columns[1]] == service][self.columns[0]].tolist()\n",
        "                    if users_of_service:\n",
        "                        ratings = [self.dataset[(self.dataset[self.columns[0]] == u) & (self.dataset[self.columns[1]] == service)][self.columns[2]].iloc[0]\n",
        "                                    for u in users_of_service if u in self.users_ratings.index]\n",
        "                        if ratings:\n",
        "                            qos_predictions.append(np.mean(ratings))\n",
        "\n",
        "            if qos_predictions:\n",
        "                return np.mean(qos_predictions)\n",
        "            else:\n",
        "                return self.globalMean\n",
        "\n",
        "        p_u = self.P[uid]\n",
        "        q_i = self.Q[iid]\n",
        "        Y = self.Y\n",
        "\n",
        "        _sum_yj = np.zeros([1, self.number_LatentFactors])\n",
        "        jids = self.users_ratings.loc[uid]['movieId'][0]\n",
        "        Nu = len(jids)\n",
        "        for jid in jids:\n",
        "            _sum_yj += Y[jid]\n",
        "\n",
        "        user_country = self.dataset[self.dataset[self.columns[0]] == uid][self.columns[3]].iloc[0]\n",
        "        item_country = self.dataset[self.dataset[self.columns[1]] == iid][self.columns[3]].iloc[0]\n",
        "\n",
        "        c_u = self.C[user_country]\n",
        "        c_i = self.C[item_country]\n",
        "        cultural_component = np.dot(c_u, c_i)\n",
        "\n",
        "        return self.globalMean + self.bu[uid] + self.bi[iid] + np.dot(p_u + np.sqrt(1 / Nu) * _sum_yj, q_i) + cultural_component\n",
        "\n",
        "    def test(self, testset):\n",
        "        for uid, iid, real_rating, country in testset.itertuples(index=False):\n",
        "            try:\n",
        "                pred_rating = self.predict(uid, iid)\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "            else:\n",
        "                yield uid, iid, real_rating, pred_rating\n",
        "\n",
        "    def accuracy(self, predict_results):\n",
        "        def mae_rmse(predict_results):\n",
        "            length = 0\n",
        "            _mae_sum = 0\n",
        "            _rmse_sum = 0\n",
        "            for uid, iid, real_rating, pred_rating in predict_results:\n",
        "                length += 1\n",
        "                _mae_sum += abs(pred_rating - real_rating)\n",
        "                _rmse_sum += (pred_rating - real_rating) ** 2\n",
        "            return _mae_sum / length, np.sqrt(_rmse_sum / length)\n",
        "\n",
        "        mae, rmse = mae_rmse(predict_results)\n",
        "        return mae, rmse\n",
        "\n",
        "    def sgd(self):\n",
        "        P, Q, C = self._init_matrix()\n",
        "\n",
        "        bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings))))\n",
        "        bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings))))\n",
        "        Y = dict(zip(\n",
        "            self.items_ratings.index,\n",
        "            np.random.rand(len(self.items_ratings), self.number_LatentFactors).astype(np.float32)\n",
        "        ))\n",
        "\n",
        "        mae_list = []\n",
        "        rmse_list = []\n",
        "\n",
        "        for i in range(self.number_epochs):\n",
        "            print(\"iter%d\" % i)\n",
        "            error_list = []\n",
        "            for uid, iid, r_ui, country in self.dataset.itertuples(index=False):\n",
        "\n",
        "                jids = self.users_ratings.loc[uid]['movieId'][0]\n",
        "                Nu = len(jids)\n",
        "                _sum_yj = np.zeros([self.number_LatentFactors])\n",
        "\n",
        "                for jid in jids:\n",
        "                    _sum_yj += Y[jid]\n",
        "\n",
        "                user_country = self.dataset[self.dataset[self.columns[0]] == uid][self.columns[3]].iloc[0]\n",
        "                item_country = self.dataset[self.dataset[self.columns[1]] == iid][self.columns[3]].iloc[0]\n",
        "                c_u = C[user_country]\n",
        "                c_i = C[item_country]\n",
        "\n",
        "                pred = self.globalMean + bu[uid] + bi[iid] + np.dot(P[uid] + np.sqrt(1 / Nu) * _sum_yj, Q[iid]) + np.dot(c_u, c_i)\n",
        "                err = np.float32(r_ui - pred)\n",
        "\n",
        "                for jid in jids:\n",
        "                    Y[jid] += self.alpha * (err * np.sqrt(1 / Nu) * Q[iid] - 0.01 * Y[jid])\n",
        "\n",
        "                P[uid] += self.alpha * (err * Q[iid] - self.reg_p * P[uid])\n",
        "                Q[iid] += self.alpha * (err * (P[uid] + np.sqrt(1 / Nu) * _sum_yj) - self.reg_q * Q[iid])\n",
        "\n",
        "                bu[uid] += self.alpha * (err - self.reg_bu * bu[uid])\n",
        "                bi[iid] += self.alpha * (err - self.reg_bi * bi[iid])\n",
        "\n",
        "                C[user_country] += self.alpha * (err * c_i - self.reg_cultural * c_u)\n",
        "                C[item_country] += self.alpha * (err * c_u - self.reg_cultural * c_i)\n",
        "\n",
        "                error_list.append(err ** 2)\n",
        "            print(np.sqrt(np.mean(error_list)))\n",
        "            self.P = P\n",
        "            self.Q = Q\n",
        "            self.bu = bu\n",
        "            self.bi = bi\n",
        "            self.Y = Y\n",
        "            self.C = C\n",
        "\n",
        "            pred_results = self.test(self.valset)\n",
        "            mae, rmse = self.accuracy(pred_results)\n",
        "            mae_list.append(mae)\n",
        "            rmse_list.append(rmse)\n",
        "            print(\"MAE: \", mae, \"RMSE: \", rmse)\n",
        "\n",
        "        x = range(1, self.number_epochs + 1)\n",
        "        plt.plot(x, rmse_list)\n",
        "        plt.title('SVDpp_SGD with Cultural Component')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('RMSE')\n",
        "        plt.show()\n",
        "        return P, Q, bu, bi, Y, C\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    cultural_data = pd.read_excel('/content/Hofstedes Dimensions Data.xlsx')\n",
        "    trainset = pd.read_csv('/content/cleaned_qos_data.csv')\n",
        "    alpha = 0.01\n",
        "    reg_p = 0.01\n",
        "    reg_q = 0.01\n",
        "    reg_bu = 0.01\n",
        "    reg_bi = 0.01\n",
        "    reg_cultural = 0.01\n",
        "    number_LatentFactors = 10\n",
        "    number_epochs = 10\n",
        "    K = 10\n",
        "    cultural_dim_weights = [1/6]*6\n",
        "    columns=[\"userId\", \"movieId\", \"rating\", \"country\"]\n",
        "    epsilon2 = 0.5\n",
        "    epsilon3 = 0.5\n",
        "\n",
        "    algo = CulturalSVDpp(alpha, reg_p, reg_q, reg_bu, reg_bi, reg_cultural, number_LatentFactors, number_epochs, K, columns, cultural_dim_weights, epsilon2, epsilon3, cultural_dimension_variances)\n",
        "    algo.fit(trainset, valset, cultural_data)\n",
        "\n",
        "    pred_results = algo.test(valset)\n",
        "    mae, rmse = algo.accuracy(pred_results)\n",
        "    print(\"MAE: \", mae, \"RMSE: \", rmse)\n"
      ],
      "metadata": {
        "id": "WemM15QaeBDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class CulturalSVDpp(object):\n",
        "    def __init__(self, alpha, reg_p, reg_q, reg_bu, reg_bi, reg_cultural, number_LatentFactors=10, number_epochs=10, K=10,\n",
        "                 columns=[\"userId\", \"movieId\", \"rating\", \"country\"], cultural_dim_weights=[1/6]*6, epsilon2=0.5, epsilon3=0.5, cultural_dimension_variances=[1]*6):\n",
        "        self.alpha = alpha\n",
        "        self.reg_p = reg_p\n",
        "        self.reg_q = reg_q\n",
        "        self.reg_bu = reg_bu\n",
        "        self.reg_bi = reg_bi\n",
        "        self.reg_cultural = reg_cultural\n",
        "        self.number_LatentFactors = number_LatentFactors\n",
        "        self.number_epochs = number_epochs\n",
        "        self.K = K\n",
        "        self.columns = columns\n",
        "        self.cultural_dim_weights = cultural_dim_weights\n",
        "        self.epsilon2 = epsilon2\n",
        "        self.epsilon3 = epsilon3\n",
        "        self.cultural_dimension_variances = cultural_dimension_variances\n",
        "\n",
        "    def fit(self, dataset, valset, cultural_data):\n",
        "        self.dataset = pd.DataFrame(dataset)\n",
        "        self.valset = valset\n",
        "        self.cultural_data = cultural_data\n",
        "\n",
        "        self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]]\n",
        "        self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]]\n",
        "        self.globalMean = self.dataset[self.columns[2]].mean()\n",
        "\n",
        "        self.P, self.Q, self.bu, self.bi, self.Y, self.C = self.sgd()\n",
        "\n",
        "    def _init_matrix(self):\n",
        "        P = dict(zip(\n",
        "            self.users_ratings.index,\n",
        "            np.random.rand(len(self.users_ratings), self.number_LatentFactors).astype(np.float32)\n",
        "        ))\n",
        "        Q = dict(zip(\n",
        "            self.items_ratings.index,\n",
        "            np.random.rand(len(self.items_ratings), self.number_LatentFactors).astype(np.float32)\n",
        "        ))\n",
        "\n",
        "        countries = self.dataset[self.columns[3]].unique()\n",
        "        C = dict(zip(\n",
        "            countries,\n",
        "            np.random.rand(len(countries), self.number_LatentFactors).astype(np.float32)\n",
        "        ))\n",
        "        return P, Q, C\n",
        "\n",
        "    def cultural_distance(self, country1, country2):\n",
        "        if country1 not in self.cultural_data or country2 not in self.cultural_data:\n",
        "            return 0.0\n",
        "\n",
        "        country1_data = self.cultural_data[country1]\n",
        "        country2_data = self.cultural_data[country2]\n",
        "\n",
        "        distance = 0.0\n",
        "        m = len(country1_data)\n",
        "        if m != len(country2_data):\n",
        "            raise ValueError(\"Cultural dimension vectors must have the same length\")\n",
        "\n",
        "        for i in range(m):\n",
        "            distance += (self.cultural_dim_weights[i] * self.cultural_dimension_variances[i] * (country1_data[i] - country2_data[i])**2)\n",
        "\n",
        "        return np.sqrt(distance)\n",
        "\n",
        "    def cultural_similarity(self, country1, country2):\n",
        "        cd1 = self.cultural_distance(country1, country1)\n",
        "        cd2 = self.cultural_distance(country2, country2)\n",
        "        return 1 - (cd1 + cd2)\n",
        "\n",
        "    def user_preference(self, user_country, service_country):\n",
        "        cdu = self.cultural_distance(user_country, user_country)\n",
        "        cds = self.cultural_distance(service_country, service_country)\n",
        "        return abs(cdu - cds)\n",
        "\n",
        "    def select_heuristic_services(self, new_user_country):\n",
        "        similar_users = []\n",
        "        for user in self.users_ratings.index:\n",
        "            user_country = self.dataset[self.dataset[self.columns[0]] == user][self.columns[3]].iloc[0]\n",
        "            similarity = self.cultural_similarity(new_user_country, user_country)\n",
        "            similar_users.append((user, similarity))\n",
        "\n",
        "        similar_users = sorted(similar_users, key=lambda x: x[1], reverse=True)\n",
        "        similar_users = [user for user, similarity in similar_users]\n",
        "\n",
        "        preferred_services = set()\n",
        "        for user in similar_users:\n",
        "            if not self.users_ratings.index.isin([user]).any():\n",
        "                continue\n",
        "            user_country = self.dataset[self.dataset[self.columns[0]] == user][self.columns[3]].iloc[0]\n",
        "            user_services = self.dataset[self.dataset[self.columns[0]] == user][self.columns[1]].tolist()\n",
        "            for service in user_services:\n",
        "                if not self.items_ratings.index.isin([service]).any():\n",
        "                    continue\n",
        "                service_country = self.dataset[self.dataset[self.columns[1]] == service][self.columns[3]].iloc[0]\n",
        "                preference = self.user_preference(user_country, service_country)\n",
        "                if preference < self.epsilon2:\n",
        "                    preferred_services.add(service)\n",
        "\n",
        "        preferred_services = list(preferred_services)\n",
        "\n",
        "        if len(preferred_services) < self.K:\n",
        "            additional_services = set()\n",
        "            for service in preferred_services:\n",
        "                if not self.items_ratings.index.isin([service]).any():\n",
        "                    continue\n",
        "                service_country = self.dataset[self.dataset[self.columns[1]] == service][self.columns[3]].iloc[0]\n",
        "                for other_service in self.items_ratings.index:\n",
        "                    if not self.items_ratings.index.isin([other_service]).any():\n",
        "                        continue\n",
        "                    other_service_country = self.dataset[self.dataset[self.columns[1]] == other_service][self.columns[3]].iloc[0]\n",
        "                    similarity = self.cultural_similarity(service_country, other_service_country)\n",
        "                    if similarity > self.epsilon3:\n",
        "                        additional_services.add(other_service)\n",
        "\n",
        "            preferred_services.extend(list(additional_services))\n",
        "\n",
        "        return preferred_services[:self.K]\n",
        "\n",
        "    def predict(self, uid, iid):\n",
        "        if uid not in self.users_ratings.index or iid not in self.items_ratings.index:\n",
        "            try:\n",
        "                user_country = self.dataset[self.dataset[self.columns[0]] == uid][self.columns[3]].iloc[0]\n",
        "            except:\n",
        "                print(f\"User {uid} not found in dataset\")\n",
        "                return self.globalMean\n",
        "            heuristic_services = self.select_heuristic_services(user_country)\n",
        "            if not heuristic_services:\n",
        "                return self.globalMean\n",
        "\n",
        "            qos_predictions = []\n",
        "            for service in heuristic_services:\n",
        "                if service in self.items_ratings.index:\n",
        "                    users_of_service = self.dataset[self.dataset[self.columns[1]] == service][self.columns[0]].tolist()\n",
        "                    if users_of_service:\n",
        "                        ratings = [self.dataset[(self.dataset[self.columns[0]] == u) & (self.dataset[self.columns[1]] == service)][self.columns[2]].iloc[0]\n",
        "                                    for u in users_of_service if u in self.users_ratings.index and (self.dataset[(self.dataset[self.columns[0]] == u) & (self.dataset[self.columns[1]] == service)][self.columns[2]].shape[0] > 0)]\n",
        "                        if ratings:\n",
        "                            qos_predictions.append(np.mean(ratings))\n",
        "\n",
        "            if qos_predictions:\n",
        "                return np.mean(qos_predictions)\n",
        "            else:\n",
        "                return self.globalMean\n",
        "\n",
        "        p_u = self.P[uid]\n",
        "        q_i = self.Q[iid]\n",
        "        Y = self.Y\n",
        "\n",
        "        _sum_yj = np.zeros([1, self.number_LatentFactors])\n",
        "        jids = self.users_ratings.loc[uid]['movieId'][0]\n",
        "        Nu = len(jids)\n",
        "        for jid in jids:\n",
        "            _sum_yj += Y[jid]\n",
        "\n",
        "        user_country = self.dataset[self.dataset[self.columns[0]] == uid][self.columns[3]].iloc[0]\n",
        "        item_country = self.dataset[self.dataset[self.columns[1]] == iid][self.columns[3]].iloc[0]\n",
        "\n",
        "        c_u = self.C[user_country]\n",
        "        c_i = self.C[item_country]\n",
        "        cultural_component = np.dot(c_u, c_i)\n",
        "\n",
        "        return self.globalMean + self.bu[uid] + self.bi[iid] + np.dot(p_u + np.sqrt(1 / Nu) * _sum_yj, q_i) + cultural_component\n",
        "\n",
        "    def test(self, testset):\n",
        "        for uid, iid, real_rating, country in testset.itertuples(index=False):\n",
        "            try:\n",
        "                pred_rating = self.predict(uid, iid)\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "            else:\n",
        "                yield uid, iid, real_rating, pred_rating\n",
        "\n",
        "    def accuracy(self, predict_results):\n",
        "        def mae_rmse(predict_results):\n",
        "            length = 0\n",
        "            _mae_sum = 0\n",
        "            _rmse_sum = 0\n",
        "            for uid, iid, real_rating, pred_rating in predict_results:\n",
        "                length += 1\n",
        "                _mae_sum += abs(pred_rating - real_rating)\n",
        "                _rmse_sum += (pred_rating - real_rating) ** 2\n",
        "            return _mae_sum / length, np.sqrt(_rmse_sum / length)\n",
        "\n",
        "        mae, rmse = mae_rmse(predict_results)\n",
        "        return mae, rmse\n",
        "\n",
        "    def sgd(self):\n",
        "        P, Q, C = self._init_matrix()\n",
        "\n",
        "        bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings))))\n",
        "        bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings))))\n",
        "        Y = dict(zip(\n",
        "            self.items_ratings.index,\n",
        "            np.random.rand(len(self.items_ratings), self.number_LatentFactors).astype(np.float32)\n",
        "        ))\n",
        "\n",
        "        mae_list = []\n",
        "        rmse_list = []\n",
        "\n",
        "        for i in range(self.number_epochs):\n",
        "            print(\"iter%d\" % i)\n",
        "            error_list = []\n",
        "            for uid, iid, r_ui, country in self.dataset.itertuples(index=False):\n",
        "\n",
        "                jids = self.users_ratings.loc[uid]['movieId'][0]\n",
        "                Nu = len(jids)\n",
        "                _sum_yj = np.zeros([self.number_LatentFactors])\n",
        "\n",
        "                for jid in jids:\n",
        "                    Y[jid] += self.alpha * (err * np.sqrt(1 / Nu) * Q[iid] - 0.01 * Y[jid])\n",
        "                    _sum_yj += Y[jid]\n",
        "\n",
        "                user_country = self.dataset[self.dataset[self.columns[0]] == uid][self.columns[3]].iloc[0]\n",
        "                item_country = self.dataset[self.dataset[self.columns[1]] == iid][self.columns[3]].iloc[0]\n",
        "                c_u = C[user_country]\n",
        "                c_i = C[item_country]\n",
        "\n",
        "                pred = self.globalMean + bu[uid] + bi[iid] + np.dot(P[uid] + np.sqrt(1 / Nu) * _sum_yj, Q[iid]) + np.dot(c_u, c_i)\n",
        "                err = np.float32(r_ui - pred)\n",
        "\n",
        "                P[uid] += self.alpha * (err * Q[iid] - self.reg_p * P[uid])\n",
        "                Q[iid] += self.alpha * (err * (P[uid] + np.sqrt(1 / Nu) * _sum_yj) - self.reg_q * Q[iid])\n",
        "\n",
        "                bu[uid] += self.alpha * (err - self.reg_bu * bu[uid])\n",
        "                bi[iid] += self.alpha * (err - self.reg_bi * bi[iid])\n",
        "\n",
        "                C[user_country] += self.alpha * (err * c_i - self.reg_cultural * c_u)\n",
        "                C[item_country] += self.alpha * (err * c_u - self.reg_cultural * c_i)\n",
        "\n",
        "                error_list.append(err ** 2)\n",
        "            print(np.sqrt(np.mean(error_list)))\n",
        "            self.P = P\n",
        "            self.Q = Q\n",
        "            self.bu = bu\n",
        "            self.bi = bi\n",
        "            self.Y = Y\n",
        "            self.C = C\n",
        "\n",
        "            pred_results = self.test(self.valset)\n",
        "            mae, rmse = self.accuracy(pred_results)\n",
        "            mae_list.append(mae)\n",
        "            rmse_list.append(rmse)\n",
        "            print(\"MAE: \", mae, \"RMSE: \", rmse)\n",
        "\n",
        "        x = range(1, self.number_epochs + 1)\n",
        "        plt.plot(x, rmse_list)\n",
        "        plt.title('SVDpp_SGD with Cultural Component')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('RMSE')\n",
        "        plt.show()\n",
        "        return P, Q, bu, bi, Y, C\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    cultural_data = pd.read_excel('/content/Hofstedes Dimensions Data.xlsx')\n",
        "    trainset = pd.read_csv('/content/cleaned_qos_data.csv')\n",
        "    #Take subset of data\n",
        "    n_users = 10\n",
        "    n_movies = 10\n",
        "    trainset = trainset[trainset['userId'] <= n_users]\n",
        "    trainset = trainset[trainset['movieId'] <= (100 + n_movies)]\n",
        "\n",
        "    alpha = 0.01\n",
        "    reg_p = 0.01\n",
        "    reg_q = 0.01\n",
        "    reg_bu = 0.01\n",
        "    reg_bi = 0.01\n",
        "    reg_cultural = 0.01\n",
        "    number_LatentFactors = 10\n",
        "    number_epochs = 10\n",
        "    K = 10\n",
        "    cultural_dim_weights = [1/6]*6\n",
        "    columns=[\"userId\", \"movieId\", \"rating\", \"country\"]\n",
        "    epsilon2 = 0.5\n",
        "    epsilon3 = 0.5\n",
        "\n",
        "    algo = CulturalSVDpp(alpha, reg_p, reg_q, reg_bu, reg_bi, reg_cultural, number_LatentFactors, number_epochs, K, columns, cultural_dim_weights, epsilon2, epsilon3, cultural_dimension_variances)\n",
        "    algo.fit(trainset, valset, cultural_data)\n",
        "\n",
        "    pred_results = algo.test(valset)\n",
        "    mae, rmse = algo.accuracy(pred_results)\n",
        "    print(\"MAE: \", mae, \"RMSE: \", rmse)\n"
      ],
      "metadata": {
        "id": "7CxD8820fcsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class CulturalSVDpp(object):\n",
        "    def __init__(self, alpha, reg_p, reg_q, reg_bu, reg_bi, reg_cultural, number_LatentFactors=10, number_epochs=10, K=10,\n",
        "                 columns=[\"userId\", \"movieId\", \"rating\", \"country\"], cultural_dim_weights=[1/6]*6, epsilon2=0.5, epsilon3=0.5, cultural_dimension_variances=[1]*6):\n",
        "        self.alpha = alpha\n",
        "        self.reg_p = reg_p\n",
        "        self.reg_q = reg_q\n",
        "        self.reg_bu = reg_bu\n",
        "        self.reg_bi = reg_bi\n",
        "        self.reg_cultural = reg_cultural\n",
        "        self.number_LatentFactors = number_LatentFactors\n",
        "        self.number_epochs = number_epochs\n",
        "        self.K = K\n",
        "        self.columns = columns\n",
        "        self.cultural_dim_weights = cultural_dim_weights\n",
        "        self.epsilon2 = epsilon2\n",
        "        self.epsilon3 = epsilon3\n",
        "        self.cultural_dimension_variances = cultural_dimension_variances\n",
        "\n",
        "    def fit(self, dataset, valset, cultural_data):\n",
        "        self.dataset = pd.DataFrame(dataset)\n",
        "        self.valset = valset\n",
        "        self.cultural_data = cultural_data\n",
        "\n",
        "        self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]]\n",
        "        self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]]\n",
        "        self.globalMean = self.dataset[self.columns[2]].mean()\n",
        "\n",
        "        self.P, self.Q, self.bu, self.bi, self.Y, self.C = self.sgd()\n",
        "\n",
        "    def _init_matrix(self):\n",
        "        P = dict(zip(\n",
        "            self.users_ratings.index,\n",
        "            np.random.rand(len(self.users_ratings), self.number_LatentFactors).astype(np.float32)\n",
        "        ))\n",
        "        Q = dict(zip(\n",
        "            self.items_ratings.index,\n",
        "            np.random.rand(len(self.items_ratings), self.number_LatentFactors).astype(np.float32)\n",
        "        ))\n",
        "\n",
        "        countries = self.dataset[self.columns[3]].unique()\n",
        "        C = dict(zip(\n",
        "            countries,\n",
        "            np.random.rand(len(countries), self.number_LatentFactors).astype(np.float32)\n",
        "        ))\n",
        "        return P, Q, C\n",
        "\n",
        "    def cultural_distance(self, country1, country2):\n",
        "        if country1 not in self.cultural_data or country2 not in self.cultural_data:\n",
        "            return 0.0\n",
        "\n",
        "        country1_data = self.cultural_data[country1]\n",
        "        country2_data = self.cultural_data[country2]\n",
        "\n",
        "        distance = 0.0\n",
        "        m = len(country1_data)\n",
        "        if m != len(country2_data):\n",
        "            raise ValueError(\"Cultural dimension vectors must have the same length\")\n",
        "\n",
        "        for i in range(m):\n",
        "            distance += (self.cultural_dim_weights[i] * self.cultural_dimension_variances[i] * (country1_data[i] - country2_data[i])**2)\n",
        "\n",
        "        return np.sqrt(distance)\n",
        "\n",
        "    def cultural_similarity(self, country1, country2):\n",
        "        cd1 = self.cultural_distance(country1, country1)\n",
        "        cd2 = self.cultural_distance(country2, country2)\n",
        "        return 1 - (cd1 + cd2)\n",
        "\n",
        "    def user_preference(self, user_country, service_country):\n",
        "        cdu = self.cultural_distance(user_country, user_country)\n",
        "        cds = self.cultural_distance(service_country, service_country)\n",
        "        return abs(cdu - cds)\n",
        "\n",
        "    def select_heuristic_services(self, new_user_country):\n",
        "        similar_users = []\n",
        "        for user in self.users_ratings.index:\n",
        "            user_country = self.dataset[self.dataset[self.columns[0]] == user][self.columns[3]].iloc[0]\n",
        "            similarity = self.cultural_similarity(new_user_country, user_country)\n",
        "            similar_users.append((user, similarity))\n",
        "\n",
        "        similar_users = sorted(similar_users, key=lambda x: x[1], reverse=True)\n",
        "        similar_users = [user for user, similarity in similar_users]\n",
        "\n",
        "        preferred_services = set()\n",
        "        for user in similar_users:\n",
        "            if not self.users_ratings.index.isin([user]).any():\n",
        "                continue\n",
        "            user_country = self.dataset[self.dataset[self.columns[0]] == user][self.columns[3]].iloc[0]\n",
        "            user_services = self.dataset[self.dataset[self.columns[0]] == user][self.columns[1]].tolist()\n",
        "            for service in user_services:\n",
        "                if not self.items_ratings.index.isin([service]).any():\n",
        "                    continue\n",
        "                service_country = self.dataset[self.dataset[self.columns[1]] == service][self.columns[3]].iloc[0]\n",
        "                preference = self.user_preference(user_country, service_country)\n",
        "                if preference < self.epsilon2:\n",
        "                    preferred_services.add(service)\n",
        "\n",
        "        preferred_services = list(preferred_services)\n",
        "\n",
        "        if len(preferred_services) < self.K:\n",
        "            additional_services = set()\n",
        "            for service in preferred_services:\n",
        "                if not self.items_ratings.index.isin([service]).any():\n",
        "                    continue\n",
        "                service_country = self.dataset[self.dataset[self.columns[1]] == service][self.columns[3]].iloc[0]\n",
        "                for other_service in self.items_ratings.index:\n",
        "                    if not self.items_ratings.index.isin([other_service]).any():\n",
        "                        continue\n",
        "                    other_service_country = self.dataset[self.dataset[self.columns[1]] == other_service][self.columns[3]].iloc[0]\n",
        "                    similarity = self.cultural_similarity(service_country, other_service_country)\n",
        "                    if similarity > self.epsilon3:\n",
        "                        additional_services.add(other_service)\n",
        "\n",
        "            preferred_services.extend(list(additional_services))\n",
        "\n",
        "        return preferred_services[:self.K]\n",
        "\n",
        "    def predict(self, uid, iid):\n",
        "        if uid not in self.users_ratings.index or iid not in self.items_ratings.index:\n",
        "            try:\n",
        "                user_country = self.dataset[self.dataset[self.columns[0]] == uid][self.columns[3]].iloc[0]\n",
        "            except:\n",
        "                print(f\"User {uid} not found in dataset\")\n",
        "                return self.globalMean\n",
        "            heuristic_services = self.select_heuristic_services(user_country)\n",
        "            if not heuristic_services:\n",
        "                return self.globalMean\n",
        "\n",
        "            qos_predictions = []\n",
        "            for service in heuristic_services:\n",
        "                if service in self.items_ratings.index:\n",
        "                    users_of_service = self.dataset[self.dataset[self.columns[1]] == service][self.columns[0]].tolist()\n",
        "                    if users_of_service:\n",
        "                        ratings = [self.dataset[(self.dataset[self.columns[0]] == u) & (self.dataset[self.columns[1]] == service)][self.columns[2]].iloc[0]\n",
        "                                    for u in users_of_service if u in self.users_ratings.index and (self.dataset[(self.dataset[self.columns[0]] == u) & (self.dataset[self.columns[1]] == service)][self.columns[2]].shape[0] > 0)]\n",
        "                        if ratings:\n",
        "                            qos_predictions.append(np.mean(ratings))\n",
        "\n",
        "            if qos_predictions:\n",
        "                return np.mean(qos_predictions)\n",
        "            else:\n",
        "                return self.globalMean\n",
        "\n",
        "        p_u = self.P[uid]\n",
        "        q_i = self.Q[iid]\n",
        "        Y = self.Y\n",
        "\n",
        "        _sum_yj = np.zeros([1, self.number_LatentFactors])\n",
        "        jids = self.users_ratings.loc[uid]['movieId'][0]\n",
        "        Nu = len(jids)\n",
        "        for jid in jids:\n",
        "            _sum_yj += Y[jid]\n",
        "\n",
        "        user_country = self.dataset[self.dataset[self.columns[0]] == uid][self.columns[3]].iloc[0]\n",
        "        item_country = self.dataset[self.dataset[self.columns[1]] == iid][self.columns[3]].iloc[0]\n",
        "\n",
        "        c_u = self.C[user_country]\n",
        "        c_i = self.C[item_country]\n",
        "        cultural_component = np.dot(c_u, c_i)\n",
        "\n",
        "        return self.globalMean + self.bu[uid] + self.bi[iid] + np.dot(p_u + np.sqrt(1 / Nu) * _sum_yj, q_i) + cultural_component\n",
        "\n",
        "    def test(self, testset):\n",
        "        for uid, iid, real_rating, country in testset.itertuples(index=False):\n",
        "            try:\n",
        "                pred_rating = self.predict(uid, iid)\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "            else:\n",
        "                yield uid, iid, real_rating, pred_rating\n",
        "\n",
        "    def accuracy(self, predict_results):\n",
        "        def mae_rmse(predict_results):\n",
        "            length = 0\n",
        "            _mae_sum = 0\n",
        "            _rmse_sum = 0\n",
        "            for uid, iid, real_rating, pred_rating in predict_results:\n",
        "                length += 1\n",
        "                _mae_sum += abs(pred_rating - real_rating)\n",
        "                _rmse_sum += (pred_rating - real_rating) ** 2\n",
        "            return _mae_sum / length, np.sqrt(_rmse_sum / length)\n",
        "\n",
        "        mae, rmse = mae_rmse(predict_results)\n",
        "        return mae, rmse\n",
        "\n",
        "    def sgd(self):\n",
        "        P, Q, C = self._init_matrix()\n",
        "\n",
        "        bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings))))\n",
        "        bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings))))\n",
        "        Y = dict(zip(\n",
        "            self.items_ratings.index,\n",
        "            np.random.rand(len(self.items_ratings), self.number_LatentFactors).astype(np.float32)\n",
        "        ))\n",
        "\n",
        "        mae_list = []\n",
        "        rmse_list = []\n",
        "\n",
        "        for i in range(self.number_epochs):\n",
        "            print(\"iter%d\" % i)\n",
        "            error_list = []\n",
        "            for uid, iid, r_ui, country in self.dataset.itertuples(index=False):\n",
        "\n",
        "                jids = self.users_ratings.loc[uid]['movieId'][0]\n",
        "                Nu = len(jids)\n",
        "                _sum_yj = np.zeros([self.number_LatentFactors])\n",
        "\n",
        "                for jid in jids:\n",
        "                    Y[jid] += self.alpha * (err * np.sqrt(1 / Nu) * Q[iid] - 0.01 * Y[jid])\n",
        "                    _sum_yj += Y[jid]\n",
        "\n",
        "                user_country = self.dataset[self.dataset[self.columns[0]] == uid][self.columns[3]].iloc[0]\n",
        "                item_country = self.dataset[self.dataset[self.columns[1]] == iid][self.columns[3]].iloc[0]\n",
        "                c_u = C[user_country]\n",
        "                c_i = C[item_country]\n",
        "\n",
        "                pred = self.globalMean + bu[uid] + bi[iid] + np.dot(P[uid] + np.sqrt(1 / Nu) * _sum_yj, Q[iid]) + np.dot(c_u, c_i)\n",
        "                err = np.float32(r_ui - pred)\n",
        "\n",
        "                P[uid] += self.alpha * (err * Q[iid] - self.reg_p * P[uid])\n",
        "                Q[iid] += self.alpha * (err * (P[uid] + np.sqrt(1 / Nu) * _sum_yj) - self.reg_q * Q[iid])\n",
        "\n",
        "                bu[uid] += self.alpha * (err - self.reg_bu * bu[uid])\n",
        "                bi[iid] += self.alpha * (err - self.reg_bi * bi[iid])\n",
        "\n",
        "                C[user_country] += self.alpha * (err * c_i - self.reg_cultural * c_u)\n",
        "                C[item_country] += self.alpha * (err * c_u - self.reg_cultural * c_i)\n",
        "\n",
        "                error_list.append(err ** 2)\n",
        "            print(np.sqrt(np.mean(error_list)))\n",
        "            self.P = P\n",
        "            self.Q = Q\n",
        "            self.bu = bu\n",
        "            self.bi = bi\n",
        "            self.Y = Y\n",
        "            self.C = C\n",
        "\n",
        "            pred_results = self.test(self.valset)\n",
        "            mae, rmse = self.accuracy(pred_results)\n",
        "            mae_list.append(mae)\n",
        "            rmse_list.append(rmse)\n",
        "            print(\"MAE: \", mae, \"RMSE: \", rmse)\n",
        "\n",
        "        x = range(1, self.number_epochs + 1)\n",
        "        plt.plot(x, rmse_list)\n",
        "        plt.title('SVDpp_SGD with Cultural Component')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('RMSE')\n",
        "        plt.show()\n",
        "        return P, Q, bu, bi, Y, C\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    cultural_data = pd.read_excel('/content/Hofstedes Dimensions Data.xlsx')\n",
        "    trainset = pd.read_csv('/content/cleaned_qos_data.csv')\n",
        "\n",
        "    #Take subset of data\n",
        "    n_users = 10\n",
        "    n_movies = 10\n",
        "    trainset = trainset[trainset['userId'] <= n_users]\n",
        "    trainset = trainset[trainset['movieId'] <= (100 + n_movies)]\n",
        "\n",
        "    alpha = 0.01\n",
        "    reg_p = 0.01\n",
        "    reg_q = 0.01\n",
        "    reg_bu = 0.01\n",
        "    reg_bi = 0.01\n",
        "    reg_cultural = 0.01\n",
        "    number_LatentFactors = 10\n",
        "    number_epochs = 10\n",
        "    K = 10\n",
        "    cultural_dim_weights = [1/6]*6\n",
        "    columns=[\"userId\", \"movieId\", \"rating\", \"country\"]\n",
        "    epsilon2 = 0.5\n",
        "    epsilon3 = 0.5\n",
        "\n",
        "    algo = CulturalSVDpp(alpha, reg_p, reg_q, reg_bu, reg_bi, reg_cultural, number_LatentFactors, number_epochs, K, columns, cultural_dim_weights, epsilon2, epsilon3, cultural_dimension_variances)\n",
        "    algo.fit(trainset, valset, cultural_data)\n",
        "\n",
        "    pred_results = algo.test(valset)\n",
        "    mae, rmse = algo.accuracy(pred_results)\n",
        "    print(\"MAE: \", mae, \"RMSE: \", rmse)\n"
      ],
      "metadata": {
        "id": "_bvVe9bKgarH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}